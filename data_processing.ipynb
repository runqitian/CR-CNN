{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import re\n",
    "e1pt = re.compile('<e1>.*</e1>')\n",
    "e2pt = re.compile('<e2>.*</e2>')\n",
    "E1LABEL = 'E1LABEL'\n",
    "E2LABEL = 'E2LABEL'\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "    # sentence\n",
    "    # entity_idx\n",
    "    # context_idx\n",
    "    # hypernyms\n",
    "    # words\n",
    "    # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 400000\n",
      "finished 390000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "if WORD_DIM == 50:\n",
    "    with open('./glove.twitter.27B.50d.txt', 'r') as rf:\n",
    "        word_dict = {}\n",
    "        lines = rf.readlines()\n",
    "        count = 0\n",
    "        print('length is {}'.format(len(lines)))\n",
    "        for line in lines:\n",
    "            ls = line[:-1].split(' ')\n",
    "            word_dict[ls[0]] = [float(item) for item in ls[1:]]\n",
    "            if count % 10000 == 0:\n",
    "                print('\\rfinished {}'.format(count), end='', flush=True)\n",
    "            count += 1\n",
    "    #         break\n",
    "        print('')\n",
    "    print(len(word_dict))\n",
    "elif WORD_DIM == 300:\n",
    "    with open('../wiki6b/glove.6B.300d.txt', 'r') as rf:\n",
    "        word_dict = {}\n",
    "        lines = rf.readlines()\n",
    "        count = 0\n",
    "        print('length is {}'.format(len(lines)))\n",
    "        for line in lines:\n",
    "            ls = line[:-1].split(' ')\n",
    "            word_dict[ls[0]] = [float(item) for item in ls[1:]]\n",
    "            if count % 10000 == 0:\n",
    "                print('\\rfinished {}'.format(count), end='', flush=True)\n",
    "            count += 1\n",
    "    #         break\n",
    "        print('')\n",
    "    print(len(word_dict))\n",
    "else:\n",
    "    print('word_dim not right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 8000\n",
      "2717 2717\n"
     ]
    }
   ],
   "source": [
    "# read text file\n",
    "def read_txt(path):\n",
    "    with open(path, 'r') as rf:\n",
    "        lines = rf.readlines()\n",
    "        li = 0\n",
    "        sentences = []\n",
    "        relations = []\n",
    "        for i in range(0,len(lines), 4):\n",
    "            sentences.append(lines[i])\n",
    "            relations.append(lines[i+1])\n",
    "    print(len(sentences), len(relations))\n",
    "    return sentences, relations\n",
    "\n",
    "train_sentences, train_relations = read_txt('./raw/TRAIN_FILE.TXT')\n",
    "test_sentences, test_relations = read_txt('./raw/TEST_FILE_FULL.TXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 ['Component-Whole', 'Other', 'Instrument-Agency']\n",
      "2717 ['Message-Topic', 'Product-Producer', 'Instrument-Agency']\n",
      "10 ['Entity-Destination', 'Entity-Origin', 'Content-Container', 'Message-Topic', 'Product-Producer', 'Member-Collection', 'Cause-Effect', 'Instrument-Agency', 'Component-Whole', 'Other']\n"
     ]
    }
   ],
   "source": [
    "# process labels\n",
    "# RELATION_NUM = 19\n",
    "OTHER_LABEL = 'Other'\n",
    "RELATION_NUM = 10\n",
    "if RELATION_NUM == 10:\n",
    "    for i in range(len(train_relations)):\n",
    "        if train_relations[i] == 'Other\\n':\n",
    "            train_relations[i] = train_relations[i][:-1]\n",
    "            continue\n",
    "        train_relations[i] = train_relations[i][:-8]\n",
    "    for i in range(len(test_relations)):\n",
    "        if test_relations[i] == 'Other\\n':\n",
    "            test_relations[i] = test_relations[i][:-1]\n",
    "            continue\n",
    "        test_relations[i] = test_relations[i][:-8]\n",
    "elif RELATION_NUM == 19:\n",
    "    for i in range(len(train_relations)):\n",
    "        train_relations[i] = train_relations[i][:-1]\n",
    "    for i in range(len(test_relations)):\n",
    "        test_relations[i] = test_relations[i][:-1]\n",
    "    \n",
    "unique_set = set(test_relations + train_relations)\n",
    "unique_set.remove(OTHER_LABEL)\n",
    "unique_relations = list(unique_set)\n",
    "unique_relations.append(OTHER_LABEL)\n",
    "print(len(train_relations), train_relations[:3])\n",
    "print(len(test_relations), test_relations[:3])\n",
    "print(len(unique_relations), unique_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 2717 [3, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "# unique_relations = ['Message-Topic', 'Instrument-Agency', 'Product-Producer', 'Content-Container', 'Entity-Origin', 'Component-Whole', 'Entity-Destination', 'Member-Collection', 'Cause-Effect',  'Other']\n",
    "rel2lb = {}\n",
    "lb2rel = {}\n",
    "for i,item in enumerate(unique_relations):\n",
    "    rel2lb[item] = i\n",
    "    lb2rel[i] = item\n",
    "train_labels = [rel2lb[item] for item in train_relations]\n",
    "test_labels = [rel2lb[item] for item in test_relations]\n",
    "print(len(train_labels), len(test_labels), test_labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence):\n",
    "    # regex replace\n",
    "    sentence = re.sub('\\d*\\t\\\"', \"\", sentence)\n",
    "    sentence = sentence[:-2]\n",
    "    e1 = e1pt.search(sentence)\n",
    "    sentence = re.sub(e1pt, E1LABEL, sentence)\n",
    "    e2 = e2pt.search(sentence)\n",
    "    sentence = re.sub(e2pt, E2LABEL, sentence)\n",
    "    \n",
    "    # split word\n",
    "    words= []\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        words.append(token.text)\n",
    "    \n",
    "    return sentence, (e1.group(0)[4:-5],e2.group(0)[4:-5]), words\n",
    "\n",
    "def process_sentence_group(org_sentences):\n",
    "    entities = []\n",
    "    words_list = []\n",
    "    processed_sentences = []\n",
    "    for i in range(len(org_sentences)):\n",
    "        sentence = org_sentences[i]\n",
    "        sentence, entity, words =  process_sentence(sentence)\n",
    "        entities.append(entity)\n",
    "        words_list.append(words)\n",
    "        processed_sentences.append(sentence)\n",
    "        if (i%100 == 0):\n",
    "            print('\\rfinished {}'.format(i),flush=True, end='')\n",
    "    print('')\n",
    "    return processed_sentences, entities, words_list\n",
    "\n",
    "train_processed, train_entities, train_words = process_sentence_group(train_sentences)\n",
    "test_processed, test_entities, test_words = process_sentence_group(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The system as described above has its greatest application in an arrayed E1LABEL of antenna E2LABEL.', 'The E1LABEL was carefully wrapped and bound into the E2LABEL by means of a cord.', 'The E1LABEL of a keygen uses a E2LABEL to look at the raw assembly code.']\n",
      "[('configuration', 'elements'), ('child', 'cradle'), ('author', 'disassembler')]\n",
      "[['The', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'E1LABEL', 'of', 'antenna', 'E2LABEL', '.'], ['The', 'E1LABEL', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', 'E2LABEL', 'by', 'means', 'of', 'a', 'cord', '.'], ['The', 'E1LABEL', 'of', 'a', 'keygen', 'uses', 'a', 'E2LABEL', 'to', 'look', 'at', 'the', 'raw', 'assembly', 'code', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(train_processed[:3])\n",
    "print(train_entities[:3])\n",
    "print(train_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "def lexical_features(org_words):\n",
    "    context_words = []\n",
    "    word_idxs = []\n",
    "    count = 0\n",
    "    for each in org_words:\n",
    "        w1 = ''\n",
    "        w2 = ''\n",
    "        w3 = ''\n",
    "        w4 = ''\n",
    "        idx1 = 0\n",
    "        idx2 = 0\n",
    "        for i in range(len(each)):\n",
    "            if each[i] == E1LABEL:\n",
    "                w1 = each[i-1] if i-1>=0 else '<padding_word>'\n",
    "                w2 = each[i+1] if i+1<len(each) else '<padding_word>'\n",
    "                idx1 = i\n",
    "            if each[i] == E2LABEL:\n",
    "                w3 = each[i-1] if i-1>=0 else '<padding_word>'\n",
    "                w4 = each[i+1] if i+1<len(each) else '<padding_word>'\n",
    "                idx2 = i\n",
    "        context_words.append((w1, w2, w3, w4))\n",
    "        word_idxs.append([idx1, idx2])\n",
    "        if count%100 == 0:\n",
    "            print('\\rfinished {}'.format(count), flush=True, end='')\n",
    "        count += 1\n",
    "    print('')\n",
    "    return context_words, word_idxs\n",
    "\n",
    "train_context,train_idxs = lexical_features(train_words)\n",
    "test_context,test_idxs = lexical_features(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arrayed', 'of', 'antenna', '.'), ('The', 'was', 'the', 'by'), ('The', 'of', 'a', 'to')]\n",
      "[[12, 15], [1, 9], [1, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(train_context[:3])\n",
    "print(train_idxs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 7900\n",
      "finished 2700\n",
      "[('configuration', 'elements'), ('child', 'cradle'), ('author', 'disassembler')]\n",
      "[['the', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'configuration', 'of', 'antenna', 'elements', '.'], ['the', 'child', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', 'cradle', 'by', 'means', 'of', 'a', 'cord', '.'], ['the', 'author', 'of', 'a', 'keygen', 'uses', 'a', 'disassembler', 'to', 'look', 'at', 'the', 'raw', 'assembly', 'code', '.']]\n"
     ]
    }
   ],
   "source": [
    "def replace_word(org_list, org_word_list):\n",
    "    new_list = []\n",
    "    for i in range(len(org_list)):\n",
    "        e1 = (nlp(org_list[i][0])[-1].text).lower()\n",
    "        e2 = (nlp(org_list[i][1])[-1].text).lower()\n",
    "        new_list.append((e1,e2))\n",
    "        for j in range(len(org_word_list[i])):\n",
    "            if org_word_list[i][j] == E1LABEL:\n",
    "                org_word_list[i][j] = e1\n",
    "            if org_word_list[i][j] == E2LABEL:\n",
    "                org_word_list[i][j] = e2\n",
    "            org_word_list[i][j] = org_word_list[i][j].lower()\n",
    "        if i%100 == 0:\n",
    "            print('\\rfinished {}'.format(i), flush=True, end='')\n",
    "    print('')\n",
    "    return new_list\n",
    "\n",
    "train_entities = replace_word(train_entities, train_words)\n",
    "test_entities = replace_word(test_entities, test_words)\n",
    "print(train_entities[:3])\n",
    "print(train_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hypernyms(org_entities):\n",
    "#     hypernyms = []\n",
    "#     for i in range(len(org_entities)):\n",
    "#         h1 = ''\n",
    "#         h2 = ''\n",
    "#         temp = wn.synsets(org_entities[i][0])\n",
    "#         if len(temp) > 0:\n",
    "#             t2 = temp[0].hypernyms()\n",
    "#             if len(t2) > 0:\n",
    "#                 h1 = t2[0].lemmas()[0].name()\n",
    "#         temp = wn.synsets(org_entities[i][1])\n",
    "#         if len(temp) > 0:\n",
    "#             t2 = temp[0].hypernyms()\n",
    "#             if len(t2) > 0:\n",
    "#                 h2 = t2[0].lemmas()[0].name()\n",
    "        \n",
    "#         h1 = h1.split('_')[-1]\n",
    "#         h2 = h2.split('_')[-1]\n",
    "#         hypernyms.append((h1, h2))\n",
    "#         if i%100 == 0:\n",
    "#             print('\\rfinished {}'.format(i), flush=True, end='')\n",
    "#     print('')\n",
    "#     return hypernyms\n",
    "\n",
    "# train_hypernyms = get_hypernyms(train_entities)\n",
    "# test_hypernyms = get_hypernyms(test_entities)\n",
    "# print(train_hypernyms[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set([w for item in train_words+test_words for w in item]))\n",
    "unique_words.append('<unknown_word>')\n",
    "unique_words.append('<padding_word>')\n",
    "unknown_idx = len(unique_words) - 2\n",
    "padding_idx = len(unique_words) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22549 22549\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for i,w in enumerate(unique_words):\n",
    "    word2idx[w] = i\n",
    "    idx2word[i] = w\n",
    "print(len(word2idx), len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14945  -0.026645  0.21009 ]\n",
      " [-0.22366  -0.49923  -0.38972 ]\n",
      " [ 0.23448  -0.28152  -0.073587]]\n",
      "1128 ['birdtown', 'pedologic', 'meinie'] 22549\n"
     ]
    }
   ],
   "source": [
    "not_in_count = 0\n",
    "not_in_words = []\n",
    "def get_word_embed(unique_words, word_dict):\n",
    "    global not_in_count, not_in_words\n",
    "    word_embed = np.zeros([WORD_DIM, len(unique_words)], dtype= 'float32')\n",
    "    for i, w in enumerate(unique_words):\n",
    "        if w not in word_dict:\n",
    "            not_in_count += 1\n",
    "            not_in_words.append(w)\n",
    "        word_embed[:, i] = word_dict[w] if w in word_dict else np.random.normal(0,1,WORD_DIM)\n",
    "    word_embed[:,-1] = np.zeros(WORD_DIM, dtype='float')\n",
    "    return word_embed\n",
    "word_embed = get_word_embed(unique_words, word_dict)\n",
    "print(word_embed[:3,:3])\n",
    "print(not_in_count, not_in_words[:3],len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 0\n",
    "for item in train_words + test_words:\n",
    "    if len(item) > MAX_LEN:\n",
    "        MAX_LEN = len(item)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WORD_DIM': 300, 'MAX_LEN': 97, 'RELATION_NUM': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = {\n",
    "    'WORD_DIM': WORD_DIM,\n",
    "    'MAX_LEN': MAX_LEN,\n",
    "    'RELATION_NUM': RELATION_NUM\n",
    "}\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 8000\n",
      "finished 2700\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "    # sentence\n",
    "    # entity_idx\n",
    "    # entity_words\n",
    "    # context_words\n",
    "    # hypernyms\n",
    "    # words\n",
    "    # label\n",
    "train_iter = zip(train_sentences, train_idxs, train_words, train_labels)\n",
    "test_iter = zip(test_sentences, test_idxs, test_words, test_labels)\n",
    "\n",
    "\n",
    "def transform_and_write_tf(data):\n",
    "    tf_ex_ser = []\n",
    "    for i,each in enumerate(data):\n",
    "        sentence = each[0]\n",
    "        idxs = [item for item in each[1]]\n",
    "        words = each[2]\n",
    "        label = each[3]\n",
    "        words = [word2idx[item] if item in word2idx else unknown_idx for item in words]\n",
    "        f1 = tf.train.Feature(bytes_list=tf.train.BytesList(value=[sentence.encode('utf-8')]))\n",
    "        f2 = tf.train.Feature(int64_list=tf.train.Int64List(value=idxs))\n",
    "        f3 = tf.train.Feature(int64_list=tf.train.Int64List(value=words))\n",
    "        f4 = tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "        feature = {\n",
    "            'sentece': f1,\n",
    "            'idxs': f2,\n",
    "            'words':f3,\n",
    "            'label':f4\n",
    "        }\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "        tf_ex_ser.append(example)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('\\rfinished {}'.format(i+1), end='', flush=True)\n",
    "    print('')\n",
    "    return tf_ex_ser\n",
    "\n",
    "\n",
    "train_tf = transform_and_write_tf(train_iter)\n",
    "test_tf = transform_and_write_tf(test_iter)\n",
    "#     print(sentence)\n",
    "#     print(idxs)\n",
    "#     print(lexical)\n",
    "#     print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "record_file = base_dir + 'train.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for example in train_tf:\n",
    "        writer.write(example)\n",
    "\n",
    "record_file = base_dir + 'test.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for example in test_tf:\n",
    "        writer.write(example)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2idx, open(base_dir + 'word2idx.dict', 'wb'))\n",
    "pickle.dump(idx2word, open(base_dir + 'idx2word.dict', 'wb'))\n",
    "pickle.dump(lb2rel, open(base_dir + 'lb2rel.dict', 'wb'))\n",
    "pickle.dump(rel2lb, open(base_dir + 'rel2lb.dict', 'wb'))\n",
    "pickle.dump(word_embed, open(base_dir + 'word_embed','wb'))\n",
    "pickle.dump(unique_relations, open(base_dir + 'unique_relations', 'wb'))\n",
    "pickle.dump(data_config, open(base_dir + 'data_config.dict', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(data_config, open(base_dir + 'data_config.dict', 'wb'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
